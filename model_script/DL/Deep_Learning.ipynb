{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score : ```0.86```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_predict():\n",
    "    for folder_name in os.listdir(\"../../Competition_data\"):\n",
    "        if os.path.exists(f\"../../Competition_data/{folder_name}/y_predict.csv\") is False: continue\n",
    "        os.remove(f\"../../Competition_data/{folder_name}/y_predict.csv\")\n",
    "# use this function to remove all the y_predict.csv\n",
    "remove_all_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "for folder_name in os.listdir(\"../../Competition_data\"):\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(pd.read_csv(f\"../../Competition_data/{folder_name}/X_train.csv\",header=0))\n",
    "    y_trains.append(pd.read_csv(f\"../../Competition_data/{folder_name}/y_train.csv\",header=0))\n",
    "    X_tests.append(pd.read_csv(f\"../../Competition_data/{folder_name}/X_test.csv\",header=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_datatype(X_data):\n",
    "    # Split the features in X_data into numeric_data and categorical_data.\n",
    "    numeric, categoric = [], []\n",
    "    for feature in X_data:\n",
    "        val = float(X_data.loc[i, feature])\n",
    "        if isinstance(val, float) and val.is_integer():\n",
    "                categoric.append(feature)\n",
    "        else:   numeric.append(feature)\n",
    "    return numeric, categoric\n",
    "\n",
    "def preprocess_numeric_data(numeric_data):\n",
    "    imputer = SimpleImputer(strategy = \"mean\")\n",
    "    imputed_numeric_data = imputer.fit_transform(numeric_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_numeric_data = scaler.fit_transform(imputed_numeric_data)\n",
    "    return scaled_numeric_data\n",
    "\n",
    "def preprocess_categoric_data(categoric_data):\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    encoded_data = encoder.fit_transform(categoric_data)\n",
    "    return pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categoric_data.columns))\n",
    "\n",
    "def align_columns(X_train, X_test):\n",
    "\n",
    "    train_cols = X_train.columns\n",
    "    test_cols = X_test.columns\n",
    "    all_cols = test_cols.union(train_cols)\n",
    "\n",
    "    test_missing_cols = train_cols.difference(test_cols)\n",
    "    train_missing_cols = test_cols.difference(train_cols)\n",
    "\n",
    "    for col in train_missing_cols:\n",
    "        X_train[col] = 0\n",
    "    for col in test_missing_cols:\n",
    "        X_test[col] = 0  \n",
    "    X_train = X_train[all_cols]\n",
    "    X_test = X_test[all_cols]\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def preprocess(X_data, numeric_features, categoric_features):\n",
    "    X_data.loc[:, numeric_features] = preprocess_numeric_data(X_data[numeric_features])\n",
    "    new_columns = preprocess_categoric_data(X_data.loc[:, categoric_features])\n",
    "    X_data = X_data.drop(columns = categoric_features).reset_index(drop = True)\n",
    "    X_data = pd.concat([X_data, new_columns], axis = 1).reset_index(drop = True)\n",
    "    return X_data\n",
    "\n",
    "for i, _ in enumerate(dataset_names):\n",
    "    X_train, X_test, Y_train = X_trains[i], X_tests[i], y_trains[i]\n",
    "\n",
    "    numeric_features, categoric_features = get_number_of_datatype(X_train)\n",
    "\n",
    "    X_train = preprocess(X_train, numeric_features, categoric_features)\n",
    "    X_test = preprocess( X_test, numeric_features, categoric_features)\n",
    "    X_trains[i], X_tests[i] = align_columns(X_train, X_test)\n",
    "    y_trains[i] = Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split & build Model\n",
    "You can select an appropriate model and perform corresponding hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:13<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  \n",
    "        self.fc2 = nn.Linear(64, 32)           \n",
    "        self.fc3 = nn.Linear(32, 1)           \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # 使用ReLU激活函數\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # 輸出層使用sigmoid激活函數\n",
    "        return x\n",
    "\n",
    "models = []\n",
    "aucs = []\n",
    "\n",
    "for i in tqdm(range(len(dataset_names))):\n",
    "    if TEST_MODEL:\n",
    "        tmp_X_train, tmp_X_test, tmp_y_train, tmp_y_test = train_test_split(\n",
    "            X_trains[i], y_trains[i], test_size = 0.2, random_state = 42)\n",
    "    else:\n",
    "        tmp_X_train, tmp_y_train = X_trains[i], y_trains[i]\n",
    "        \n",
    "    train_tensor = TensorDataset(torch.FloatTensor(tmp_X_train.values), torch.FloatTensor(tmp_y_train.values))\n",
    "    train_loader = DataLoader(train_tensor, batch_size=32, shuffle=True)\n",
    "    \n",
    "    input_size = tmp_X_train.shape[1] \n",
    "    model = SimpleNN(input_size)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    if TEST_MODEL:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tmp_y_prob = model(torch.FloatTensor(tmp_X_test.values)).numpy()\n",
    "        auc = roc_auc_score(tmp_y_test, tmp_y_prob)\n",
    "        aucs.append(auc)\n",
    "        \n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicts = []\n",
    "for i in range(len(dataset_names)):\n",
    "    model = models[i]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_predict_proba = model(torch.FloatTensor(X_tests[i].values)).numpy()\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,dataset_name in enumerate(dataset_names):\n",
    "    df = y_predicts[idx]\n",
    "    df.to_csv(f'../../Competition_data/{dataset_name}/y_predict.csv', index = False,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
